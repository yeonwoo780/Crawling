{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1346fdae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "['The Matrix is everywhere its all around us, here even in this room.', 'you can see it out your window or on your television.', 'you fell it when you go to work, or go to church or pay your taxes']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "text_sample =  'The Matrix is everywhere its all around us, here even in this room. \\\n",
    "                you can see it out your window or on your television. \\\n",
    "                you fell it when you go to work, or go to church or pay your taxes'\n",
    "\n",
    "sentences = sent_tokenize(text = text_sample)\n",
    "\n",
    "print(type(sentences), len(sentences))\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "042bab3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 15\n",
      "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "sentence =  'The Matrix is everywhere its all around us, here even in this room.'\n",
    "\n",
    "words = word_tokenize(sentence)\n",
    "print(type(words), len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e00506e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['you', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['you', 'fell', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes']]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens\n",
    "\n",
    "text_sample =  'The Matrix is everywhere its all around us, here even in this room. \\\n",
    "                you can see it out your window or on your television. \\\n",
    "                you fell it when you go to work, or go to church or pay your taxes'\n",
    "\n",
    "word_tokens = tokenize_text(text_sample)\n",
    "print(type(word_tokens), len(word_tokens))\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c015abc",
   "metadata": {},
   "source": [
    "#### stopwords 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "644c29ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fa4f8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 stop words 갯수: 179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    }
   ],
   "source": [
    "print('영어 stop words 갯수:', len(nltk.corpus.stopwords.words('english')))\n",
    "print(nltk.corpus.stopwords.words('english')[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87ad26d",
   "metadata": {},
   "source": [
    "#### stopwords 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "648a8bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['fell', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes']]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "all_tokens = []\n",
    "\n",
    "for sentence in word_tokens:\n",
    "    filtered_words = []\n",
    "    for word in sentence:\n",
    "        word = word.lower()\n",
    "        \n",
    "        if word not in stopwords:\n",
    "            filtered_words.append(word)\n",
    "            \n",
    "    all_tokens.append(filtered_words)\n",
    "    \n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3451e74",
   "metadata": {},
   "source": [
    "#### Stemming과 Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ec1b729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amus amus amus\n",
      "happy happy\n",
      "fant fant\n",
      "exect exect exect\n"
     ]
    }
   ],
   "source": [
    "# 동사로 분리해줌\n",
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "print(stemmer.stem('working'),stemmer.stem('works'),stemmer.stem('worked'))\n",
    "print(stemmer.stem('amusing'),stemmer.stem('amuses'),stemmer.stem('amused'))\n",
    "print(stemmer.stem('happier'),stemmer.stem('happiese'))\n",
    "print(stemmer.stem('fancier'),stemmer.stem('fanciese'))\n",
    "print(stemmer.stem('execting'), stemmer.stem('exected'), stemmer.stem('exectly'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7868b86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 비교급 최상급은 a로 동사 불러와주고 \n",
    "### 일반 형태들은 v로 동사 불러와주는 듯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13f956b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amuse amuse amuse\n",
      "happy happy\n",
      "fancy fancy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "print(lemma.lemmatize('working','v'),lemma.lemmatize('works','v'),lemma.lemmatize('worked','v'))\n",
    "print(lemma.lemmatize('amusing','v'),lemma.lemmatize('amuses','v'),lemma.lemmatize('amused','v'))\n",
    "print(lemma.lemmatize('happier','a'),lemma.lemmatize('happiest','a'))\n",
    "print(lemma.lemmatize('fancier','a'),lemma.lemmatize('fanciest','a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4638cab4",
   "metadata": {},
   "source": [
    "### 희소 행렬"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60c01be",
   "metadata": {},
   "source": [
    "#### COO형식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98b764c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dense = np.array([[3, 0, 1], [0, 2, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f3d1d4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "data = np.array([3, 1, 2])\n",
    "\n",
    "row_pos = np.array([0, 0, 1])\n",
    "col_pos = np.array([0, 2, 1])\n",
    "\n",
    "sparse_coo = sparse.coo_matrix((data, (row_pos, col_pos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14338e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 1],\n",
       "       [0, 2, 0]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_coo.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5227be80",
   "metadata": {},
   "source": [
    "#### CSR 형식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f179b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "dense2 = np.array([[0, 0, 1, 0, 0, 5],\n",
    "                    [1, 4, 0, 3, 2, 5],\n",
    "                    [0, 6, 0, 3, 0, 0],\n",
    "                    [2, 0, 0, 0, 0, 0],\n",
    "                    [0, 0, 0, 7, 0, 6],\n",
    "                    [1, 0, 0, 0, 0, 0]])\n",
    "\n",
    "data2 = np.array([1, 5, 1, 4, 3, 2, 5 ,6 ,3, 2, 7, 8, 1])\n",
    "\n",
    "row_pos = np.array([0, 0, 1, 1, 1, 1, 1, 2, 2, 3, 4, 4, 5])\n",
    "col_pos = np.array([2, 5, 0, 1, 3, 4, 5, 1, 3, 0, 3, 5, 0])\n",
    "\n",
    "sparse_coo = sparse.coo_matrix((data2, (row_pos, col_pos)))\n",
    "\n",
    "row_pos_ind = np.array([0, 2, 7, 9, 10, 12, 13])\n",
    "\n",
    "sparse_csr = sparse.csr_matrix((data2, col_pos, row_pos_ind))\n",
    "\n",
    "print(sparse_coo.toarray())\n",
    "\n",
    "print(sparse_csr.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d627c30",
   "metadata": {},
   "source": [
    "#### 간단히 표현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "78fc6388",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense3 = np.array([[0, 0, 1, 0, 0, 5],\n",
    "                    [1, 4, 0, 3, 2, 5],\n",
    "                    [0, 6, 0, 3, 0, 0],\n",
    "                    [2, 0, 0, 0, 0, 0],\n",
    "                    [0, 0, 0, 7, 0, 6],\n",
    "                    [1, 0, 0, 0, 0, 0]])\n",
    "\n",
    "coo = sparse.coo_matrix(dense3)\n",
    "csr = sparse.coo_matrix(dense3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a2b47d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
